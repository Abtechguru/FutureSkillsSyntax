# monitoring/elk/logstash/pipeline/01-inputs.conf
input {
  # Filebeat input
  beats {
    port => 5000
    host => "0.0.0.0"
    ssl => false
  }
  
  # TCP input for structured logs
  tcp {
    port => 5001
    codec => json_lines
    type => "json_logs"
  }
  
  # Syslog input
  udp {
    port => 514
    type => "syslog"
  }
}

# monitoring/elk/logstash/pipeline/10-filters.conf
filter {
  # Parse JSON logs
  if [type] == "json_logs" or [message] =~ /^{.*}$/ {
    json {
      source => "message"
      target => "json_content"
    }
    
    # Move fields to root
    if [json_content] {
      ruby {
        code => '
          event.get("json_content").each do |key, value|
            event.set(key, value)
          end
          event.remove("json_content")
        '
      }
    }
  }
  
  # Parse Docker logs
  if [container][name] {
    mutate {
      add_field => { 
        "[@metadata][application]" => "%{[container][name]}"
      }
    }
    
    # Parse application-specific logs
    if [container][name] =~ /futureskills-backend/ {
      grok {
        match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} \| %{LOGLEVEL:level} \| %{DATA:logger} \| %{GREEDYDATA:message}" }
      }
    }
    
    if [container][name] =~ /futureskills-frontend/ {
      mutate {
        add_tag => ["frontend"]
      }
    }
  }
  
  # Parse timestamp
  date {
    match => [ "timestamp", "ISO8601" ]
    target => "@timestamp"
    timezone => "UTC"
  }
  
  # Add geoip information
  if [client_ip] {
    geoip {
      source => "client_ip"
      target => "geoip"
    }
  }
  
  # Add user agent parsing
  if [user_agent] {
    useragent {
      source => "user_agent"
      target => "user_agent_info"
    }
  }
  
  # Remove sensitive data
  mutate {
    remove_field => [
      "[headers][authorization]",
      "[password]",
      "[token]",
      "[credit_card]",
      "[ssn]"
    ]
  }
  
  # Add environment tag
  mutate {
    add_field => {
      "[@metadata][environment]" => "production"
      "[@metadata][project]" => "FutureSkillsSyntax"
    }
  }
}

# monitoring/elk/logstash/pipeline/30-outputs.conf
output {
  # Send to Elasticsearch with index per day
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "futureskills-%{+YYYY.MM.dd}"
    document_type => "_doc"
    template => "/usr/share/logstash/templates/elasticsearch-template.json"
    template_name => "futureskills"
    template_overwrite => true
    
    # Retry policy
    retry_initial_backoff => 2
    retry_max_interval => 64
    retry_on_conflict => 5
    
    # Bulk settings
    flush_size => 5000
    idle_flush_time => 10
    
    # Authentication (if enabled)
    # user => "elastic"
    # password => "${ELASTIC_PASSWORD}"
  }
  
  # Debug output (only in development)
  if [@metadata][environment] == "development" {
    stdout {
      codec => rubydebug
    }
  }
  
  # Alert on critical errors
  if [level] == "ERROR" or [level] == "FATAL" {
    http {
      url => "http://alertmanager:9093/api/v1/alerts"
      http_method => "post"
      format => "json"
      mapping => {
        "alerts" => [
          {
            "labels" => {
              "alertname" => "ApplicationError",
              "severity" => "critical",
              "instance" => "%{host}",
              "application" => "%{[@metadata][application]}"
            },
            "annotations" => {
              "summary" => "Application error detected",
              "description" => "%{message}",
              "timestamp" => "%{@timestamp}"
            }
          }
        ]
      }
    }
  }
}